% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{subfigure}
\usepackage{caption}
\usepackage{float} 
\usepackage{subcaption}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.https://www.overleaf.com/project/626e0c405e417026abed7b3a
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\usepackage{graphicx}
\usepackage{subfigure}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Imaging with Spike Camera}

\author{Zeng Fu\\
School of Mathematical Sciences\\
Peking University\\
{\tt\small 1900010644@pku.edu.cn}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Liying Wang\\
School of EECS\\
Peking University\\
{\tt\small 2000012958@stu.pku.edu.cn}\\
\and
Yuhan Xing\\
Yuanpei College\\
Peking University\\
{\tt\small 2000017797@stu.pku.edu.cn}
\and
Songkun Zhan\\
School of EECS\\
Peking University\\
{\tt\small 2000013139@stu.pku.edu.cn}
}
\maketitle
%%%%%%%%% ABSTRACT
\begin{abstract}
  This project aims to calculate and generate images, which should be clear, stable and of high quality, from the given continuous spike data sequence. It is required that the output images have twice the resolution of the spike sequence. To obtain better result, we compared three methods: texture from latencies (TFL), texture from playback (TFP) and optical flow. We also tried a series of optimizations for the second and the third methods, respectively. Finally, the optical flow method yielded relatively ideal results.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

Spike camera captures light and accumulates the converted luminance intensity at each pixel. It records the pulses generated by incident lights at a speed of 20000-40000 frames per second. A spike is fired when the accumulated intensity exceeds the dispatch threshold. The spike stream generated by the camera indicates the luminance variation. Analyzing the patterns of the spike stream makes it possible to reconstruct the picture of any moment, which enables the playback of high speed movement. If we collect all the spikes fired at the same time and draw points according to the x- and y-addresses of the spikes on a clear screen, the scene structure and the object movement can be illustrated as time goes by. This is how the dynamic vision sensor (DVS) camera works.

However, the spike sequence only represents changes of luminance intensity in the temporal dimension, but can't reflect the difference of luminance intensity between a pixel and pixels around it (i.e., changes in the spacial dimension). Therefore, the texture is missing without analog-to-digital converter (ADC) data being transmitted along with the spike. Another deficiency of the DVS is that it has difficulty in processing still objects, because the luminance intensity does not change much if there is no object moving, leading to no spike to be fired. [1]

On the basis of previous work, we tried and optimized three methods -- TFL, TPL and optical flow -- to generate high-quality images from the given spike sequence. We read five related papers: "Spike Camera and Its Coding Methods" [1], "Reconstructing Clear Image for High-Speed Motion Scene with a Retina-Inspired Camera" [2], "Motion Estimation for Spike Camera Data Sequence via Spike Interval Analysis" [3], "Large Displacement Optical Flow: Descriptor Matching in Variational Motion Estimation" [4], "Kernel Regression for Image Processing and Reconstruction" [5]. Firstly, the spike sequence is interpolated to get images with a resolution of 500 × 800 × 4000. Then, we adopt some methods, which we consider intuitive and viable, to process the images. After comparing the effects of these methods, we decide on the optical flow method. Based on the original version of flow, we make optimizations and obtain significant improvements.

The rest of the report is organized as follows. Section 2 introduces two basic methods. Section 3 presents some preliminary optimizations we try. Section 4 introduces optical flow and presents our optimizations. Section 5 shows experimental results and Section 6 concludes the projects.

%-------------------------------------------------------------------------
\section{Two basic methods}

\subsection{Texture from latencies (TFL)}

The main idea of TFL is using the reciprocal of the interval between two adjacent spikes to represent the greyscale of all points in the interval. Let $i_1, i_2, ..., i_n$ be the sequence of time then spikes are fired. Assuming $i_0 = 0$, then for each $i$ such that $i_{k-1} < i \leqslant i_k$

Following the principle of TFL, the computed values are not perfectly matched with the original texture data, but the variation is in accordance with the original one. TFL can reconstruct the outline of the texture but not clear details, and there is much noise in the generated images. [1]

\subsection{Texture from playback (TFP)}

To reduce the noise in the images, we try to take advantage of the information from adjacent frames to denoise a certain frame. Given that the spikes are dispatched with a very high frequency, if we play back the spikes, the historical pictures are able to be illustrated. In TFP, there is a sliding time window collecting the spikes in a specific period. [1] The greyscale value at time $i$ is computed as:
    $$g(i) = \frac{1}{min(i+hwl,T_{max})-max(1,i-hwl)+1}\times$$
    $$\sum_{j=max(1,i-hwl)}^min(i+hwl,T_{max}) f(j)$$

Thereinto, $hwl$ is a variable parameter representing half the length of sliding window, and $T_{max}$ is the time o fthe last frame.

%-------------------------------------------------------------------------
\section{Preliminary optimizations}

\subsection{Noise reduction for single frames}

We notice that there are obvious noise points in not only the generated video, but also every single frame. Using the method in the paper [5], we denoise each frame respectively. Since the original data is binary, satisfactory results may not be acquired through direct denoising. We decide to first utilize TFL to convert the binary data into greyscale values, and do noise reduction for single frames based on this.

It it worth noting that the data-adapted kernel presented in reference paper has a considerable computational cost, but can't achieve significant improvements. Thus we make a compromise to adopt the non-adaptive kernel. To be specific, we adopt the 2-D Gaussian Kernel shown in Fig.1.

\begin{figure}[htp]
\includegraphics[width=8cm]{pic/1.jpg}
\caption{2-D Gaussian Kernel}
\end{figure}


This method had a good effect on denoising within a single frame, but it is not the focus of this project. Under the task of video denoising, we expect to make the best of information from frames in adjacent times.

\subsection{Weighting in the temporal dimension}

The TFP mentioned in {\bf Section 2.1} is relatively simple. For each frame in a fixed-length window before and after a certain frame, we give equal weights. But i reality, we expect that the nearer a frame is from the target frame, the bigger role it plays in the estimation. In order to penalize the points which have a location offset from a particular time, and to reduce adverse effects they have on the estimation, we present a sliding window with weights on the temporal dimension on the basis of TFP. The kernel function we choose is 1-D Gaussian Kernel. The function \emph{calcKer} in the code package is used to calculate 1-D Gaussian Kernel vector, while the function \emph{weightSpikeMatrix} calls \emph{calcKer} to generate appropriate kernel and do the weighted computation.

The sliding window with time weights plays a good denoising effect. Meanwhile, it effectively reduces the blur caused by equal weights. But essentially, it is still TFP. When the window is too long, it will face the same problems as the original TFP.

\subsection{Iteration}

Each method presented above has its own advantages and disadvantages: TFL does not average in the time dimension, resulting in sharp images. But noise in single frames and the whole video cannot be ignored. Original TFP or the one with time weights can generate videos that have smooth transition between adjacent frames, while the adverse effect is that the picture appears to be a certain degree of blur. To combine the advantages of each method, we do combinations and iterations of them. Experiments show that when using the iterative process as shown in Fig.2 and iterating 2 or 3 times, good effects can be achieved.

\begin{figure}[htp]
\includegraphics[width=8cm]{pic/2.png}
\caption{Iteration}
\end{figure}

\subsection{3-D Kernel regression}

We try to refer to the method presented in the paper [5], directly extending the 2-D kernel regression to the 3-D form.

If we stack the frames in time order, then we can get a "3-D" image, as is shown in Fig.3. Intuitively, like in the 2-D form, this image has its corner, edge and flat areas, and we expect the 3-D kernel to contract at the local corner texture so as to avoid the interference caused by points of great differences, to elongate along the directions of the local edge texture, and to fully spread in the flat area to make use of as much information as possible to denoise.

This extension is mathematically achievable and we have written the corresponding code (the function \emph{kernel\_regression} in the code package). However, the computational cost of 3-D kernel is so huge that we have to give it up.

\begin{figure}[htp]
\includegraphics[width=8cm]{pic/3.jpg}
\caption{3-D image}
\end{figure}

%-------------------------------------------------------------------------
\section{Optical flow and optimizations}

Optical flow is an optimization algorithm based on the sliding window. As time goes by, objects in adjacent frames are displaced, so the same point corresponds to different physical objects in different frames. That’s why sliding window results in blurry images. 

We first calculate the displacement of objects in adjacent frames. According to the displacement vector, we can get the position of every point at a new time, and compute the weighted average in the time dimension. Below is the calculation formula, where $\Vec{x}=(x,y)$ is the coordinate of the point, $\Vec{v}(i,j,x)$ is the displacement variable. And the function \emph{mex\_of} in the code package gives the specific code.
$$g(i,\Vec{x}) = \frac{1}{min(i+hwl,T_{max}-max(1,i-hwl)+1}\times$$
$$\sum_{j=max(1,i-hwl)}^{min(i+hwl,T_{max})} f(j,\Vec{x}+\Vec{v}(i,j,x))$$

\subsection{Preprocessing of the video}

When calculating the displacement, we find that if we use unprocessed videos, the displacement vectors are quite inaccurate. The reason is that the function \emph{mex\_of} has certain requirements on the quality of the video. Accordingly, we apply some methods to process the video before calculating the displacement vectors, then a more accurate result can be achieved.
 
\subsection{Reduction of computational cost}

Since the speed of most objects does not change within a few frames, there is no need to calculate the displacement vector for every frame. Rather, it will be enough to calculate the displacement vector over a certain time interval. Such simplification greatly reduces the calculation time, and the formula becomes:
$$g(i,\Vec{x}) = \frac{1}{2\times hwl+1}\times$$
$$[\sum_{j=1}^{hwl} f(i+j,\Vec{x}+\Vec{v}(i,i+l,x)\times\frac{j}{l})+$$
$$f(i-j,\Vec{x}+\Vec{v}(i,i-l,x)\times\frac{j}{l})+f(i,\Vec{x})]$$

\subsection{Elimination of unreal outlines}

We notice that although optical flow can make objects in motion clearer, there are some unreal black outlines at the edge of the objects. After discussing and researching, we conjecture that this is because the function \emph{mex\_of} wrongly estimates the displacement of points which are going to be blocked or are just exposed. Let’s consider the following situation: An object was originally moving at a certain speed. When it is gradually blocked by some other object, the algorithm will still try to calculate an “appropriate” position for it and generate the new image as if there were no block. But this out to be erroneous.
To verify our conjecture, we carry out the following experiment: We design three scenarios: (1) A white rectangle moving slowly against a black background (the corresponding code is \emph{NoWall.m} in the code package); (2) A white rectangle moving slowly against a black background and being blocked by a grey wall (\emph{WithWall.m}); (3) A white rectangle running through the picture, moving slowly against a black background and being blocked by a grey wall (\emph{WithWallThrough.m}). Among the three scenarios, (3) is closer to the situation described above. Three phenomena are observed in our experiments: (a) The velocities of the edges of objects are roughly accurate without block; (b) The velocities of the interior of objects will slow down or even tend to 0; (c) From before being blocked to after, the velocities of the blocked parts significantly changed, but the change values are uncertain.
From these experiments, we find that the edges are often stationary for half of the time, and make some unreasonable displacements for the other half. To solve this problem, we write a function (the function \emph{judge} in the file \emph{fastflow.m}) to determine whether a pixel is blocked, and remove the blocked time period from the weighting. Now the formula is:
$$g(i,\Vec{x}) = \frac{1}{2\times hwl+1}\times\{judge(\Vec{v}(i,i+l,x),\Vec{v}(i,i-l,x))$$
$$[\sum_{j=1}^{hwl} f(i+j,\Vec{x}+\Vec{v}(i,i+l,x)\times\frac{j}{l})+$$
$$f(i-j,\Vec{x}+\Vec{v}(i,i-l,x)\times\frac{j}{l})+f(i,\Vec{x})]\}$$
\emph{judge} is defined as follows:
$$judge(\Vec{a},\Vec{b})=\begin{cases} (1,1)^T, a=b=\Vec{0}, or    a\neq\Vec{0},b\neq\Vec{0}\\ (2,0)^T, a=\Vec{0},b\neq\Vec{0}\\ (0,2)^T,a\neq\Vec{0},b=\Vec{0} \end{cases}$$

\section{Experimental results}

Using the same video, we experiment with the methods mentioned. For TFP and its optimized version, we also tried different window lengths to compared the effects. Fig.4 – Fig.14 show the results and their comparison.

\section{Conclusion}

In this project, we try and compare several methods to generated high quality results from the given spike sequences. We also attempt to do a series of optimizations. Through many experiments and adjustments, we draw the conclusion that optical flow is the relatively optimal method. For further work, more efficient algorithms for 3-D kernel regression and optical flow need to be explored. Moreover, how to enhance the adaptability of these methods in different situations is worth studying.

\section{Reference}

[1] S. Dong, T. Huang and Y. Tian. “Spike Camera and Its Coding Methods,” \emph{Data Compression Conference (DCC),} pp. 437-437, 2017, doi: 10.1109/DCC.2017.69.
[2] J. Zhao, R. Xiong, J. Xie, B. Shi, Z. Yu, W. Gao, and T. Huang. “Reconstructing Clear Image for High-Speed Motion Scene with a Retina-Inspired Spike Camera,” in \emph{IEEE Transactions on Computational Imaging,} vol. 8, pp. 12-27, 2022, doi: 10.1109/TCI.2021.3136446.
[3] J. Zhao, R. Xiong, R. Zhao, J. Wang, S. Ma and T. Huang, "Motion Estimation for Spike Camera Data Sequence via Spike Interval Analysis," \emph{2020 IEEE International Conference on Visual Communications and Image Processing (VCIP),} 2020, pp. 371-374, doi: 10.1109/VCIP49819.2020.9301840.
[4] T. Brox and J. Malik, "Large Displacement Optical Flow: Descriptor Matching in Variational Motion Estimation," in \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence,} vol. 33, no. 3, pp. 500-513, March 2011, doi: 10.1109/TPAMI.2010.143.
[5] H. Takeda, S. Farsiu and P. Milanfar, "Kernel Regression for Image Processing and Reconstruction," in \emph{IEEE Transactions on Image Processing,} vol. 16, no. 2, pp. 349-366, Feb. 2007, doi: 10.1109/TIP.2006.888330.

\begin{figure}[htp]
\includegraphics[width=8cm]{pic/4.jpg}
\caption{TFL}
%\end{figure}

%\begin{figure}[htp]
\includegraphics[width=8cm]{pic/5.1.jpg}
\caption{TFP, window length = 9}

\includegraphics[width=8cm]{pic/5.2.jpg}
\caption{TFP, window length = 17}

\includegraphics[width=8cm]{pic/5.3.jpg}
\caption{TFP, window length = 65}
\end{figure}

\begin{figure}[htp]
\includegraphics[width=8cm]{pic/6.jpg}
\caption{2-D kernel denoising}
%\end{figure}

%\begin{figure}[htp]
\includegraphics[width=8cm]{pic/7.1.jpg}
\caption{weighted TFP, window length = 9}

\includegraphics[width=8cm]{pic/7.2.jpg}
\caption{weighted TFP, window length = 17}

\includegraphics[width=8cm]{pic/7.3.jpg}
\caption{weighted TFP, window length = 65}
\end{figure}

\begin{figure}[htp]
\includegraphics[width=8cm]{pic/8.jpg}
\caption{Iteration twice}
%\end{figure}

%\begin{figure}[htp]
\includegraphics[width=8cm]{pic/9.1.jpg}
\caption{Flow, window length = 17}

\includegraphics[width=8cm]{pic/9.2.jpg}
\caption{Flow, window length = 65}

\end{figure}

\end{document}
